{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Releasing all cameras...\n",
      "Found camera objects: ['CSICamera', 'JetRacerCamera']\n",
      "‚úì Stopped CSICamera.running\n",
      "‚úì Deleted CSICamera\n",
      "‚úì Stopped JetRacerCamera.running\n",
      "Warning cleaning JetRacerCamera: JetRacerCamera.stop() missing 1 required positional argument: 'self'\n",
      "‚úì Garbage collection completed\n",
      "‚è≥ Waiting for camera hardware to release...\n",
      "‚úÖ Camera release complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst-nvarguscamera/gstnvarguscamerasrc.cpp, threadExecute:734 NvBufSurfaceFromFd Failed.\n",
      "Error generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst-nvarguscamera/gstnvarguscamerasrc.cpp, threadFunction:245 (propagating)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GST_ARGUS: Creating output stream\n",
      "CONSUMER: Waiting until producer is connected...\n",
      "GST_ARGUS: Available Sensor modes :\n",
      "GST_ARGUS: 3280 x 2464 FR = 21.000000 fps Duration = 47619048 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 3280 x 1848 FR = 28.000001 fps Duration = 35714284 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1920 x 1080 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1640 x 1232 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1280 x 720 FR = 59.999999 fps Duration = 16666667 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: Running with following settings:\n",
      "   Camera index = 0 \n",
      "   Camera mode  = 4 \n",
      "   Output Stream W = 1280 H = 720 \n",
      "   seconds to Run    = 0 \n",
      "   Frame Rate = 59.999999 \n",
      "GST_ARGUS: Setup Complete, Starting captures for 0 seconds\n",
      "GST_ARGUS: Starting repeat capture requests.\n",
      "CONSUMER: Producer has connected; continuing.\n",
      "nvbuf_utils: dmabuf_fd -1 mapped entry NOT found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@9.215] global cap_gstreamer.cpp:1728 open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not initialize camera.  Please see error trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jetcam-0.0.0-py3.10.egg/jetcam/csi_camera.py:24\u001b[0m, in \u001b[0;36mCSICamera.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not read image from camera.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not read image from camera.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Step 3: Create camera for your use case\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m camera \u001b[38;5;241m=\u001b[39m \u001b[43mJetRacerCamera\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or 'inference' or 'safe'\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Step 4: Start camera with error handling\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m camera\u001b[38;5;241m.\u001b[39mstart():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jetracer-0.0.0-py3.10.egg/jetracer/camera_utils.py:30\u001b[0m, in \u001b[0;36mJetRacerCamera.__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Same reliable config for training\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera \u001b[38;5;241m=\u001b[39m \u001b[43mCSICamera\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_fps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)  \u001b[38;5;66;03m# Resize for model input\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jetcam-0.0.0-py3.10.egg/jetcam/csi_camera.py:26\u001b[0m, in \u001b[0;36mCSICamera.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not read image from camera.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not initialize camera.  Please see error trace.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcap\u001b[38;5;241m.\u001b[39mrelease)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not initialize camera.  Please see error trace."
     ]
    }
   ],
   "source": [
    "# Step 1: Clean slate\n",
    "from jetracer.camera_utils import JetRacerCamera, release_cam\n",
    "import time\n",
    "\n",
    "# Step 2: Release any existing cameras\n",
    "release_cam()\n",
    "time.sleep(2)\n",
    "\n",
    "# Step 3: Create camera for your use case\n",
    "camera = JetRacerCamera('training')  # or 'inference' or 'safe'\n",
    "\n",
    "# Step 4: Start camera with error handling\n",
    "if camera.start():\n",
    "    print(\"‚úÖ Camera initialized successfully!\")\n",
    "    print(f\"Resolution: {camera.width}x{camera.height}\")\n",
    "    print(f\"Mode: {camera.mode}\")\n",
    "    \n",
    "    # Test image capture\n",
    "    img = camera.read()\n",
    "    if img is not None:\n",
    "        print(f\"‚úÖ Capturing images: {img.shape}\")\n",
    "        print(\"üéØ Camera ready for use!\")\n",
    "    else:\n",
    "        print(\"‚ùå Camera not capturing images\")\n",
    "else:\n",
    "    print(\"‚ùå Camera failed to initialize\")\n",
    "    print(\"üîß Try restarting Jupyter kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (224, 224, 3)\n",
      "Image type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Get the current frame from camera\n",
    "image = camera.value\n",
    "\n",
    "# Display image info\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Image type: {type(image)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from xy_dataset import XYDataset\n",
    "\n",
    "TASK = 'road_following'\n",
    "\n",
    "CATEGORIES = ['apex']\n",
    "\n",
    "DATASETS = ['A', 'B']\n",
    "\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "datasets = {}\n",
    "for name in DATASETS:\n",
    "    datasets[name] = XYDataset(TASK + '_' + name, CATEGORIES, TRANSFORMS, random_hflip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xy_dataset.XYDataset at 0xfffeb176f970>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[DATASETS[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ JETRACER DATA COLLECTION SETUP\n",
      "========================================\n",
      "üîÑ Releasing all cameras...\n",
      "Found camera objects: ['JetRacerCamera']\n",
      "‚úì Stopped JetRacerCamera.running\n",
      "Warning cleaning JetRacerCamera: JetRacerCamera.stop() missing 1 required positional argument: 'self'\n",
      "‚úì Garbage collection completed\n",
      "‚è≥ Waiting for camera hardware to release...\n",
      "‚úÖ Camera release complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error generated. /dvs/git/dirty/git-master_linux/multimedia/nvgstreamer/gst-nvarguscamera/gstnvarguscamerasrc.cpp, execute:805 Failed to create CaptureSession\n",
      "[ WARN:0@227.745] global cap_gstreamer.cpp:1728 open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not initialize camera.  Please see error trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jetcam-0.0.0-py3.10.egg/jetcam/csi_camera.py:24\u001b[0m, in \u001b[0;36mCSICamera.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not read image from camera.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not read image from camera.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m release_cam()\n\u001b[1;32m     18\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m camera \u001b[38;5;241m=\u001b[39m \u001b[43mJetRacerCamera\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 224x224 for training\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m camera\u001b[38;5;241m.\u001b[39mstart():\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Camera failed to start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jetracer-0.0.0-py3.10.egg/jetracer/camera_utils.py:30\u001b[0m, in \u001b[0;36mJetRacerCamera.__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Same reliable config for training\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera \u001b[38;5;241m=\u001b[39m \u001b[43mCSICamera\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_fps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)  \u001b[38;5;66;03m# Resize for model input\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jetcam-0.0.0-py3.10.egg/jetcam/csi_camera.py:26\u001b[0m, in \u001b[0;36mCSICamera.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not read image from camera.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not initialize camera.  Please see error trace.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcap\u001b[38;5;241m.\u001b[39mrelease)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not initialize camera.  Please see error trace."
     ]
    }
   ],
   "source": [
    "# Working JetRacer Data Collection Interface\n",
    "# Run this in your Jupyter notebook\n",
    "\n",
    "import cv2\n",
    "import ipywidgets\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import uuid\n",
    "from IPython.display import display, clear_output\n",
    "from jetracer.camera_utils import JetRacerCamera, release_cam\n",
    "\n",
    "print(\"üéØ JETRACER DATA COLLECTION SETUP\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Step 1: Initialize camera\n",
    "release_cam()\n",
    "time.sleep(2)\n",
    "\n",
    "camera = JetRacerCamera('training')  # 224x224 for training\n",
    "if not camera.start():\n",
    "    print(\"‚ùå Camera failed to start\")\n",
    "    exit()\n",
    "\n",
    "print(\"‚úÖ Camera initialized for data collection\")\n",
    "\n",
    "# Step 2: Set up data collection parameters\n",
    "CATEGORIES = ['apex']  # Road following target points\n",
    "DATASETS = ['A', 'B']  # Different data collection sessions\n",
    "\n",
    "# Create data directories\n",
    "base_dir = \"/home/checker/Autonomous-Racecar/data/training_images\"\n",
    "for dataset in DATASETS:\n",
    "    for category in CATEGORIES:\n",
    "        os.makedirs(f\"{base_dir}/road_following_{dataset}/{category}\", exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Data directories ready: {base_dir}\")\n",
    "\n",
    "# Step 3: Data collection class\n",
    "class DataCollector:\n",
    "    def __init__(self):\n",
    "        self.current_dataset = DATASETS[0]\n",
    "        self.current_category = CATEGORIES[0]\n",
    "        self.count = 0\n",
    "        self.collecting = False\n",
    "        \n",
    "    def get_save_path(self):\n",
    "        return f\"{base_dir}/road_following_{self.current_dataset}/{self.current_category}\"\n",
    "    \n",
    "    def get_count(self):\n",
    "        path = self.get_save_path()\n",
    "        if os.path.exists(path):\n",
    "            return len([f for f in os.listdir(path) if f.endswith('.jpg')])\n",
    "        return 0\n",
    "    \n",
    "    def save_image(self, image, x, y):\n",
    "        # Save image with click coordinates in filename\n",
    "        filename = f\"{x}_{y}_{str(uuid.uuid4())}.jpg\"\n",
    "        filepath = os.path.join(self.get_save_path(), filename)\n",
    "        \n",
    "        # Save the image\n",
    "        cv2.imwrite(filepath, image)\n",
    "        \n",
    "        # Update count\n",
    "        self.count = self.get_count()\n",
    "        print(f\"üíæ Saved: {filename} (Total: {self.count})\")\n",
    "        \n",
    "        return filepath\n",
    "\n",
    "# Initialize data collector\n",
    "collector = DataCollector()\n",
    "\n",
    "# Step 4: Create interactive widgets\n",
    "print(\"üéõÔ∏è Creating interactive interface...\")\n",
    "\n",
    "# Image display widget\n",
    "image_widget = ipywidgets.Image(\n",
    "    format='jpeg',\n",
    "    width=400,\n",
    "    height=400,\n",
    ")\n",
    "\n",
    "# Control widgets\n",
    "dataset_widget = ipywidgets.Dropdown(\n",
    "    options=DATASETS,\n",
    "    value=DATASETS[0],\n",
    "    description='Dataset:'\n",
    ")\n",
    "\n",
    "category_widget = ipywidgets.Dropdown(\n",
    "    options=CATEGORIES,\n",
    "    value=CATEGORIES[0],\n",
    "    description='Category:'\n",
    ")\n",
    "\n",
    "count_widget = ipywidgets.IntText(\n",
    "    value=collector.get_count(),\n",
    "    description='Count:',\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "# Click coordinates display\n",
    "click_info = ipywidgets.HTML(value=\"<b>Click on the image to collect data</b>\")\n",
    "\n",
    "# Collection status\n",
    "status_widget = ipywidgets.HTML(value=\"<b>Status: Ready</b>\")\n",
    "\n",
    "# Step 5: Update functions\n",
    "def update_dataset(change):\n",
    "    collector.current_dataset = change['new']\n",
    "    collector.count = collector.get_count()\n",
    "    count_widget.value = collector.count\n",
    "    status_widget.value = f\"<b>Status: Dataset {collector.current_dataset} selected</b>\"\n",
    "\n",
    "def update_category(change):\n",
    "    collector.current_category = change['new']\n",
    "    collector.count = collector.get_count()\n",
    "    count_widget.value = collector.count\n",
    "    status_widget.value = f\"<b>Status: Category {collector.current_category} selected</b>\"\n",
    "\n",
    "dataset_widget.observe(update_dataset, names='value')\n",
    "category_widget.observe(update_category, names='value')\n",
    "\n",
    "# Step 6: Camera feed and click handling\n",
    "def update_camera_feed():\n",
    "    \"\"\"Update camera feed continuously\"\"\"\n",
    "    while collector.collecting:\n",
    "        try:\n",
    "            img = camera.read()\n",
    "            if img is not None:\n",
    "                # Convert BGR to RGB for display\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Encode as JPEG\n",
    "                _, buffer = cv2.imencode('.jpg', img_rgb)\n",
    "                image_widget.value = buffer.tobytes()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Camera feed error: {e}\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(0.1)  # 10 FPS display\n",
    "\n",
    "# Click handler for data collection\n",
    "def handle_click(x, y, width, height):\n",
    "    \"\"\"Handle click on image for data collection\"\"\"\n",
    "    try:\n",
    "        # Get current camera image\n",
    "        img = camera.read()\n",
    "        if img is not None:\n",
    "            # Scale click coordinates to image size\n",
    "            img_x = int((x / width) * camera.width)\n",
    "            img_y = int((y / height) * camera.height)\n",
    "            \n",
    "            # Save the image with click coordinates\n",
    "            filepath = collector.save_image(img, img_x, img_y)\n",
    "            \n",
    "            # Update displays\n",
    "            count_widget.value = collector.count\n",
    "            click_info.value = f\"<b>Last click: ({img_x}, {img_y}) - Saved!</b>\"\n",
    "            status_widget.value = f\"<b>Status: Saved image #{collector.count}</b>\"\n",
    "            \n",
    "            # Show preview with click point\n",
    "            preview_img = img.copy()\n",
    "            cv2.circle(preview_img, (img_x, img_y), 8, (0, 255, 0), 3)\n",
    "            preview_rgb = cv2.cvtColor(preview_img, cv2.COLOR_BGR2RGB)\n",
    "            _, buffer = cv2.imencode('.jpg', preview_rgb)\n",
    "            image_widget.value = buffer.tobytes()\n",
    "            \n",
    "        else:\n",
    "            status_widget.value = \"<b>Status: No camera image available</b>\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        status_widget.value = f\"<b>Status: Error - {e}</b>\"\n",
    "\n",
    "# Step 7: JavaScript for click handling\n",
    "click_handler_js = \"\"\"\n",
    "<script>\n",
    "function setupClickHandler() {\n",
    "    const images = document.querySelectorAll('.widget-image img');\n",
    "    images.forEach(img => {\n",
    "        img.style.cursor = 'crosshair';\n",
    "        img.onclick = function(event) {\n",
    "            const rect = img.getBoundingClientRect();\n",
    "            const x = event.clientX - rect.left;\n",
    "            const y = event.clientY - rect.top;\n",
    "            \n",
    "            // Send click to Python\n",
    "            const kernel = Jupyter.notebook.kernel;\n",
    "            kernel.execute(`handle_click(${x}, ${y}, ${rect.width}, ${rect.height})`);\n",
    "        };\n",
    "    });\n",
    "}\n",
    "\n",
    "// Setup click handler after a short delay\n",
    "setTimeout(setupClickHandler, 1000);\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "# Step 8: Control buttons\n",
    "start_button = ipywidgets.Button(description=\"Start Collection\")\n",
    "stop_button = ipywidgets.Button(description=\"Stop Collection\")\n",
    "\n",
    "def start_collection(b):\n",
    "    collector.collecting = True\n",
    "    start_button.disabled = True\n",
    "    stop_button.disabled = False\n",
    "    status_widget.value = \"<b>Status: Collecting data - Click on image!</b>\"\n",
    "    \n",
    "    # Start camera feed thread\n",
    "    threading.Thread(target=update_camera_feed, daemon=True).start()\n",
    "\n",
    "def stop_collection(b):\n",
    "    collector.collecting = False\n",
    "    start_button.disabled = False\n",
    "    stop_button.disabled = True\n",
    "    status_widget.value = \"<b>Status: Collection stopped</b>\"\n",
    "\n",
    "start_button.on_click(start_collection)\n",
    "stop_button.on_click(stop_collection)\n",
    "\n",
    "# Step 9: Create layout\n",
    "controls = ipywidgets.VBox([\n",
    "    dataset_widget,\n",
    "    category_widget,\n",
    "    count_widget,\n",
    "    ipywidgets.HBox([start_button, stop_button]),\n",
    "    status_widget,\n",
    "    click_info\n",
    "])\n",
    "\n",
    "layout = ipywidgets.HBox([\n",
    "    image_widget,\n",
    "    controls\n",
    "])\n",
    "\n",
    "# Step 10: Display everything\n",
    "print(\"üéØ Data Collection Interface Ready!\")\n",
    "print(\"\\nInstructions:\")\n",
    "print(\"1. Click 'Start Collection'\")\n",
    "print(\"2. Click on the camera image where you want the car to go\")\n",
    "print(\"3. Images will be saved automatically with click coordinates\")\n",
    "print(\"4. Use different datasets (A, B) for different training sessions\")\n",
    "\n",
    "display(layout)\n",
    "\n",
    "# Add click handler JavaScript\n",
    "from IPython.display import HTML\n",
    "display(HTML(click_handler_js))\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete! Start collecting data by clicking the button above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/checker/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/checker/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616b34913a494fe1985c92951e5078ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='road_following_model.pth', description='model path'), HBox(children=(Button(descrip‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda')\n",
    "output_dim = 2 * len(dataset.categories)  # x, y coordinate for each category\n",
    "\n",
    "# ALEXNET\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "# model.classifier[-1] = torch.nn.Linear(4096, output_dim)\n",
    "\n",
    "# SQUEEZENET \n",
    "# model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "# model.classifier[1] = torch.nn.Conv2d(512, output_dim, kernel_size=1)\n",
    "# model.num_classes = len(dataset.categories)\n",
    "\n",
    "# RESNET 18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# RESNET 34\n",
    "# model = torchvision.models.resnet34(pretrained=True)\n",
    "# model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# DENSENET 121\n",
    "# model = torchvision.models.densenet121(pretrained=True)\n",
    "# model.classifier = torch.nn.Linear(model.num_features, output_dim)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model_save_button = ipywidgets.Button(description='save model')\n",
    "model_load_button = ipywidgets.Button(description='load model')\n",
    "model_path_widget = ipywidgets.Text(description='model path', value='road_following_model.pth')\n",
    "\n",
    "def load_model(c):\n",
    "    model.load_state_dict(torch.load(model_path_widget.value))\n",
    "model_load_button.on_click(load_model)\n",
    "    \n",
    "def save_model(c):\n",
    "    torch.save(model.state_dict(), model_path_widget.value)\n",
    "model_save_button.on_click(save_model)\n",
    "\n",
    "model_widget = ipywidgets.VBox([\n",
    "    model_path_widget,\n",
    "    ipywidgets.HBox([model_load_button, model_save_button])\n",
    "])\n",
    "\n",
    "\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f1c028f5ef4f118bcff33c3866799a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'', format='jpeg', height='720', width='1280'), ToggleButtons(description='state',‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "from utils import preprocess\n",
    "import torch.nn.functional as F\n",
    "\n",
    "state_widget = ipywidgets.ToggleButtons(options=['stop', 'live'], description='state', value='stop')\n",
    "prediction_widget = ipywidgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "\n",
    "def live(state_widget, model, camera, prediction_widget):\n",
    "    global dataset\n",
    "    while state_widget.value == 'live':\n",
    "        image = camera.value\n",
    "        preprocessed = preprocess(image)\n",
    "        output = model(preprocessed).detach().cpu().numpy().flatten()\n",
    "        category_index = dataset.categories.index(category_widget.value)\n",
    "        x = output[2 * category_index]\n",
    "        y = output[2 * category_index + 1]\n",
    "        \n",
    "        x = int(camera.width * (x / 2.0 + 0.5))\n",
    "        y = int(camera.height * (y / 2.0 + 0.5))\n",
    "        \n",
    "        prediction = image.copy()\n",
    "        prediction = cv2.circle(prediction, (x, y), 8, (255, 0, 0), 3)\n",
    "        prediction_widget.value = bgr8_to_jpeg(prediction)\n",
    "            \n",
    "def start_live(change):\n",
    "    if change['new'] == 'live':\n",
    "        execute_thread = threading.Thread(target=live, args=(state_widget, model, camera, prediction_widget))\n",
    "        execute_thread.start()\n",
    "\n",
    "state_widget.observe(start_live, names='value')\n",
    "\n",
    "live_execution_widget = ipywidgets.VBox([\n",
    "    prediction_widget,\n",
    "    state_widget\n",
    "])\n",
    "\n",
    "display(live_execution_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73e53bed9bd4ead862d918324dc37f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntText(value=1, description='epochs'), FloatProgress(value=0.0, description='progress', max=1.‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "epochs_widget = ipywidgets.IntText(description='epochs', value=1)\n",
    "eval_button = ipywidgets.Button(description='evaluate')\n",
    "train_button = ipywidgets.Button(description='train')\n",
    "loss_widget = ipywidgets.FloatText(description='loss')\n",
    "progress_widget = ipywidgets.FloatProgress(min=0.0, max=1.0, description='progress')\n",
    "\n",
    "def train_eval(is_training):\n",
    "    global BATCH_SIZE, LEARNING_RATE, MOMENTUM, model, dataset, optimizer, eval_button, train_button, accuracy_widget, loss_widget, progress_widget, state_widget\n",
    "    \n",
    "    try:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        state_widget.value = 'stop'\n",
    "        train_button.disabled = True\n",
    "        eval_button.disabled = True\n",
    "        time.sleep(1)\n",
    "\n",
    "        if is_training:\n",
    "            model = model.train()\n",
    "        else:\n",
    "            model = model.eval()\n",
    "\n",
    "        while epochs_widget.value > 0:\n",
    "            i = 0\n",
    "            sum_loss = 0.0\n",
    "            error_count = 0.0\n",
    "            for images, category_idx, xy in iter(train_loader):\n",
    "                # send data to device\n",
    "                images = images.to(device)\n",
    "                xy = xy.to(device)\n",
    "\n",
    "                if is_training:\n",
    "                    # zero gradients of parameters\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # execute model to get outputs\n",
    "                outputs = model(images)\n",
    "\n",
    "                # compute MSE loss over x, y coordinates for associated categories\n",
    "                loss = 0.0\n",
    "                for batch_idx, cat_idx in enumerate(list(category_idx.flatten())):\n",
    "                    loss += torch.mean((outputs[batch_idx][2 * cat_idx:2 * cat_idx+2] - xy[batch_idx])**2)\n",
    "                loss /= len(category_idx)\n",
    "\n",
    "                if is_training:\n",
    "                    # run backpropogation to accumulate gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # step optimizer to adjust parameters\n",
    "                    optimizer.step()\n",
    "\n",
    "                # increment progress\n",
    "                count = len(category_idx.flatten())\n",
    "                i += count\n",
    "                sum_loss += float(loss)\n",
    "                progress_widget.value = i / len(dataset)\n",
    "                loss_widget.value = sum_loss / i\n",
    "                \n",
    "            if is_training:\n",
    "                epochs_widget.value = epochs_widget.value - 1\n",
    "            else:\n",
    "                break\n",
    "    except e:\n",
    "        pass\n",
    "    model = model.eval()\n",
    "\n",
    "    train_button.disabled = False\n",
    "    eval_button.disabled = False\n",
    "    state_widget.value = 'live'\n",
    "    \n",
    "train_button.on_click(lambda c: train_eval(is_training=True))\n",
    "eval_button.on_click(lambda c: train_eval(is_training=False))\n",
    "    \n",
    "train_eval_widget = ipywidgets.VBox([\n",
    "    epochs_widget,\n",
    "    progress_widget,\n",
    "    loss_widget,\n",
    "    ipywidgets.HBox([train_button, eval_button])\n",
    "])\n",
    "\n",
    "display(train_eval_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following widget can be used to label a multi-class x, y dataset.  It supports labeling only one instance of each class per image (ie: only one dog), but multiple classes (ie: dog, cat, horse) per image are possible.\n",
    "\n",
    "Click the image on the top left to save an image of ``category`` to ``dataset`` at the clicked location.\n",
    "\n",
    "| Widget | Description |\n",
    "|--------|-------------|\n",
    "| dataset | Selects the active dataset |\n",
    "| category | Selects the active category |\n",
    "| epochs | Sets the number of epochs to train for |\n",
    "| train | Trains on the active dataset for the number of epochs specified |\n",
    "| evaluate | Evaluates the accuracy on the active dataset over one epoch |\n",
    "| model path | Sets the active model path |\n",
    "| load | Loads a model from the active model path |\n",
    "| save | Saves a model to the active model path |\n",
    "| stop | Disables the live demo |\n",
    "| live | Enables the live demo |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4b412b8a5546c2b536a3028fea937a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(HTML(value='<h3>JetRacer Data Collection</h3>'), HBox(children=(V‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([data_collection_widget, live_execution_widget]), \n",
    "    train_eval_widget,\n",
    "    model_widget\n",
    "])\n",
    "\n",
    "display(all_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
